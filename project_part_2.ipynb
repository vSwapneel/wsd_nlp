{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def generate_prompt_file_with_examples(input_file, output_file=\"prompt_file_with_examples.txt\"):\n",
    "    \"\"\"\n",
    "    Read an input file with words and their synsets, retrieve meanings and example sentences,\n",
    "    and generate a new prompt file with word, synset, definition, and example sentence.\n",
    "    \"\"\"\n",
    "    with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            \n",
    "            word, synset_name = line.strip().split(\",\")\n",
    "            try:\n",
    "            \n",
    "                synset = wn.synset(synset_name)\n",
    "                # Retrieve the definition\n",
    "                definition = synset.definition()\n",
    "                # Retrieve an example sentence or provide a placeholder\n",
    "                examples = synset.examples()\n",
    "                example_sentence = examples[0] if examples else f\"No example available for {word}.\"\n",
    "                # Write the word, synset, definition, and example sentence to the output file\n",
    "                outfile.write(f\"{word},{synset_name},{definition},{example_sentence}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {word}, {synset_name}: {e}\")\n",
    "    print(f\"Enhanced prompt file generated at: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Generating prompt file using input file</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced prompt file generated at: prompt_file_with_examples.txt\n"
     ]
    }
   ],
   "source": [
    "input_file = \"input_file.txt\"\n",
    "generate_prompt_file_with_examples(input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Retrieving more unique examples from wordnet and appending them to inpput file </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/szele/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Generate  examples from WordNet\n",
    "output_file = \"prompt_file_append.txt\"\n",
    "\n",
    "unique_words = set()  # To ensure uniqueness\n",
    "with open(output_file, \"w\") as file:\n",
    "    for synset in list(wn.all_synsets('n')): \n",
    "        if len(unique_words) >= 1000: #we can change the number for the number of examples\n",
    "            break\n",
    "        word = synset.lemmas()[0].name()\n",
    "        if word in unique_words:\n",
    "            continue\n",
    "        unique_words.add(word)\n",
    "        definition = synset.definition()\n",
    "        example_sentence = synset.examples()[0] if synset.examples() else f\"No example available for {word}.\"\n",
    "        synset_id = synset.name()\n",
    "        line = f\"{word},{synset_id},{definition},{example_sentence}\\n\"\n",
    "        file.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Model fine tuning below</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 valid prompts from the file.\n",
      "Dataset size: 1000\n",
      "{'input_text': \"Generate sentences using the word 'entity' in the sense of 'entity.n.01', which means: that which is perceived or known or inferred to have its own distinct existence (living or nonliving).\", 'output_text': 'No example available for entity.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2193.98 examples/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset size: 1000\n",
      "{'input_text': \"Generate sentences using the word 'entity' in the sense of 'entity.n.01', which means: that which is perceived or known or inferred to have its own distinct existence (living or nonliving).\", 'output_text': 'No example available for entity.', 'input_ids': [0, 40025, 877, 11305, 634, 5, 2136, 128, 46317, 108, 11, 5, 1472, 9, 128, 46317, 4, 282, 4, 2663, 3934, 61, 839, 35, 14, 61, 16, 9568, 50, 684, 50, 42870, 7, 33, 63, 308, 11693, 8066, 36, 26111, 50, 786, 26111, 322, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 3084, 1246, 577, 13, 10014, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='201' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [201/250 23:33 < 05:47, 0.14 it/s, Epoch 0.80/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.618700</td>\n",
       "      <td>4.859058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 77/125 02:49 < 01:47, 0.45 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Initialize BART Pretrained Model and Tokenizer\n",
    "model_name = \"facebook/bart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def load_prompt_file_with_examples(prompt_file):\n",
    "    \"\"\"\n",
    "    Load the enhanced prompt file and convert it into a dataset.\n",
    "\n",
    "    Args:\n",
    "        prompt_file (str): Path to the enhanced prompt file.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Hugging Face Dataset containing input-output pairs.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    with open(prompt_file, \"r\") as file:\n",
    "        for line_num, line in enumerate(file, start=1):\n",
    "            try:\n",
    "                # Ensure splitting works for four fields\n",
    "                word, synset_name, definition, example_sentence = line.strip().split(\",\", maxsplit=3)\n",
    "                input_text = (\n",
    "                    f\"Generate sentences using the word '{word}' in the sense of '{synset_name}', \"\n",
    "                    f\"which means: {definition}.\"\n",
    "                )\n",
    "                output_text = example_sentence\n",
    "                prompts.append({\"input_text\": input_text, \"output_text\": output_text})\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping line {line_num}: {line.strip()} - {e}\")\n",
    "    print(f\"Loaded {len(prompts)} valid prompts from the file.\")\n",
    "    return Dataset.from_list(prompts)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset for BART fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): Hugging Face Dataset containing input-output pairs.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Tokenized dataset.\n",
    "    \"\"\"\n",
    "    def preprocess(batch):\n",
    "        inputs = tokenizer(\n",
    "            batch[\"input_text\"], truncation=True, padding=\"max_length\", max_length=512\n",
    "        )\n",
    "        outputs = tokenizer(\n",
    "            batch[\"output_text\"], truncation=True, padding=\"max_length\", max_length=512\n",
    "        )\n",
    "        batch[\"input_ids\"] = inputs[\"input_ids\"]\n",
    "        batch[\"attention_mask\"] = inputs[\"attention_mask\"]\n",
    "        batch[\"labels\"] = outputs[\"input_ids\"]\n",
    "        return batch\n",
    "\n",
    "    return dataset.map(preprocess, batched=True)\n",
    "\n",
    "def fine_tune_bart(dataset, output_dir=\"bart_finetuned\", epochs=1):\n",
    "    \"\"\"\n",
    "    Fine-tune the BART model.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): Tokenized dataset for training.\n",
    "        output_dir (str): Directory to save the fine-tuned model.\n",
    "        epochs (int): Number of epochs for training.\n",
    "\n",
    "    Returns:\n",
    "        BartForConditionalGeneration: The fine-tuned model.\n",
    "    \"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_steps=500,\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        learning_rate=5e-4,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "prompt_file = \"prompt_file_append.txt\"\n",
    "output_dir = \"bart_finetuned\"\n",
    "\n",
    "#Load and preprocess the dataset\n",
    "dataset = load_prompt_file_with_examples(prompt_file)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(dataset[0])  # Inspect the first input-output pair\n",
    "\n",
    "tokenized_dataset = preprocess_dataset(dataset)\n",
    "print(f\"Tokenized dataset size: {len(tokenized_dataset)}\")\n",
    "print(tokenized_dataset[0])  # Inspect a tokenized pair\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tuned_model = fine_tune_bart(tokenized_dataset, output_dir=output_dir, epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Sentence generation code</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load the fine-tuned model\n",
    "fine_tuned_model_dir = \"bart_finetuned\"\n",
    "tokenizer = BartTokenizer.from_pretrained(fine_tuned_model_dir)\n",
    "model = BartForConditionalGeneration.from_pretrained(fine_tuned_model_dir)\n",
    "\n",
    "def generate_sentences_from_model(word, synset_name, definition, num_sentences=3):\n",
    "    \"\"\"\n",
    "    Generate sentences using the fine-tuned BART model.\n",
    "\n",
    "    Args:\n",
    "        word (str): The target word.\n",
    "        synset_name (str): The WordNet synset name.\n",
    "        definition (str): The definition of the word in the given synset.\n",
    "        num_sentences (int): Number of sentences to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated sentences.\n",
    "    \"\"\"\n",
    "    # Prepare the input prompt\n",
    "    prompt = (\n",
    "        f\"Generate {num_sentences} distinct sentences using the word '{word}' \"\n",
    "        f\"in the sense of '{synset_name}', which means: {definition}. \"\n",
    "        \"Make sure each sentence is simple and unique.\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate sentences\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=200,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=num_sentences,\n",
    "        do_sample=True,  # Enable sampling\n",
    "        temperature=0.7,  # Adjust randomness\n",
    "        top_k=50,  # Nucleus sampling\n",
    "        top_p=0.95,  # Nucleus sampling\n",
    "    )\n",
    "\n",
    "    # Decode and return sentences\n",
    "    sentences = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return sentences\n",
    "\n",
    "# Function to test the model with a single word and synset\n",
    "def test_model(word, synset_name, definition, num_sentences=3):\n",
    "    print(f\"Testing model for word: '{word}' and synset: '{synset_name}'\")\n",
    "    print(f\"Definition: {definition}\")\n",
    "    print(\"\\nGenerated Sentences:\")\n",
    "    \n",
    "    sentences = generate_sentences_from_model(word, synset_name, definition, num_sentences)\n",
    "    for i, sentence in enumerate(sentences, 1):\n",
    "        print(f\"{i}. {sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "openi api code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_synset_details(word, synset_name):\n",
    "    \"\"\"\n",
    "    Fetch details of the synset: definition and examples.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to find the synset for.\n",
    "        synset_name (str): The WordNet synset name (e.g., 'dog.n.01').\n",
    "\n",
    "    Returns:\n",
    "        dict: Synset details including definition and examples.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        synset = wn.synset(synset_name)\n",
    "        return {\n",
    "            \"word\": word,\n",
    "            \"definition\": synset.definition(),\n",
    "            \"examples\": synset.examples()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching synset details: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model for word: 'dog' and synset: 'dog.n.01'\n",
      "Definition: A member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds.\n",
      "\n",
      "Generated Sentences:\n",
      "1. No. partisan. Jew Jew. Jew. Fed. Jew extra. Jew host.. extra.. Jew. freeze. Jew..... externalToEVA.chieve..\",\"...chieve.tained.Coun.tained. externalToEVACoun.Coun..roud externalToEVA..Coun. host. externalToEVA.ARGET.roud.\"},. innings.ention.DS.ulpt...CounCoun‐tained hostCoun\"},.chieve.tained..paioroud. hostactionDate.\"}, inningsmbudsmanactionDatechievenesium..assed Jew.isure...\",\"antaention.assed.\n",
      "2. No. partisan. Jew Jew. Jew. Fed. Jew extra. Jew host.. extra.. Jew. freeze. Jew..... externalToEVA.chieve..\",\"...chieve.tained.Coun.tained. externalToEVACoun.Coun..roud externalToEVA..Coun. host. externalToEVA.ARGET.roud.\"},. innings.ention.DS.ulpt...CounCoun‐tained hostCoun\"},.chieve.tained..paioroud. hostactionDate.\"}, inningsmbudsmanactionDatechievenesium..assed Jew.isure...\",\"antaention.assed..\n",
      "3. No. partisan. Jew Jew. Jew. Fed. Jew extra. Jew host.. extra.. Jew. freeze. Jew..... externalToEVA.chieve..\",\"...chieve.tained.Coun.tained. externalToEVACoun.Coun..roud externalToEVA..Coun. host. externalToEVA.ARGET.roud.\"},. innings.ention.DS.ulpt...CounCoun‐tained hostCoun\"},.chieve.tained..paioroud. hostactionDate.\"}, inningsmbudsmanactionDatechievenesium..assed Jew.isure...\",\"antaention.entionCoun\n"
     ]
    }
   ],
   "source": [
    "# Test the model with the word \"dog\"\n",
    "test_model(\n",
    "    word=\"dog\",\n",
    "    synset_name=\"dog.n.01\",\n",
    "    definition=\"A member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds.\",\n",
    "    num_sentences=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. My neighbor's dog, a large golden retriever, loves to play fetch in the park.\n",
      "2. I adopted a small, stray dog that I found wandering in my neighborhood.\n",
      "3. The little girl was afraid of the big, black dog that lived next door.\n",
      "1. The small, fluffy dog wagged its tail excitedly as its owner approached.\n",
      "2. The Labrador, a popular breed of dog, is known for its friendly and outgoing nature.\n",
      "3. The dog dug a hole in the backyard, burying its favorite toy for safekeeping.\n",
      "1. The small child giggled as the playful dog licked her face.\n",
      "2. The loyal dog stayed by its owner's side even during the storm.\n",
      "3. After a long day at work, the man was greeted by the excited barks of his dog.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "openai.api_key = \"\"\n",
    "def get_synset_details(word, synset_name):\n",
    "    \"\"\"\n",
    "    Fetch details of the synset: definition and examples.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to find the synset for.\n",
    "        synset_name (str): The WordNet synset name (e.g., 'dog.n.01').\n",
    "\n",
    "    Returns:\n",
    "        dict: Synset details including definition and examples.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        synset = wn.synset(synset_name)\n",
    "        return {\n",
    "            \"word\": word,\n",
    "            \"definition\": synset.definition(),\n",
    "            \"examples\": synset.examples() if synset.examples() else [f\"No example available for {word}.\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching synset details: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_sentences(word, synset_name, num_sentences=3):\n",
    "    \"\"\"\n",
    "    Generate sentences using GPT-4 based on a word's WordNet synset meaning.\n",
    "\n",
    "    Args:\n",
    "        word (str): The target word.\n",
    "        synset_name (str): The WordNet synset name.\n",
    "        num_sentences (int): Number of sentences to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated sentences.\n",
    "    \"\"\"\n",
    "    # Fetch synset details\n",
    "    details = get_synset_details(word, synset_name)\n",
    "    if not details:\n",
    "        return [f\"Error: Synset details not found for {word}, {synset_name}.\"]\n",
    "\n",
    "    # Create a structured prompt\n",
    "    prompt = (\n",
    "        f\"Generate {num_sentences} distinct sentences using the word '{word}' in the sense of '{synset_name}', \"\n",
    "        f\"which means: {details['definition']}. \"\n",
    "        \"Here are some examples to guide you: \"\n",
    "        f\"{' '.join(details['examples'])} \"\n",
    "        \"Make sure each sentence is simple, unique, and reflects this meaning.\"\n",
    "    )\n",
    "\n",
    "    # GPT-4 API with structured prompts\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant specializing in creating contextually accurate sentences.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=300,\n",
    "        n=num_sentences\n",
    "    )\n",
    "\n",
    "    sentences = [choice[\"message\"][\"content\"] for choice in response[\"choices\"]]\n",
    "    return sentences\n",
    "\n",
    "# Example usage\n",
    "word = \"dog\"\n",
    "synset_name = \"dog.n.01\"\n",
    "generated_sentences = generate_sentences(word, synset_name, num_sentences=3)\n",
    "\n",
    "print(\"\\n\".join(generated_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
