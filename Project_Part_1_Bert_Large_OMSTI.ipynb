{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909141f9-4fc8-4977-b7f8-6ba47aca5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9a90b0-385e-4737-b6a5-c280496c96ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670dfffb-4f6d-41de-a514-67acfe0acd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/svaidya4/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/svaidya4/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc4ce3f-f318-4539-a0b5-ea8cc8fd1131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "from io import StringIO\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Helper function to get WordNet synset\n",
    "def get_synset(lemma, pos):\n",
    "    \"\"\"\n",
    "    Map a lemma and POS to its corresponding WordNet synset.\n",
    "    \"\"\"\n",
    "    pos_map = {\n",
    "        \"NOUN\": wn.NOUN,\n",
    "        \"VERB\": wn.VERB,\n",
    "        \"ADJ\": wn.ADJ,\n",
    "        \"ADV\": wn.ADV\n",
    "    }\n",
    "    wn_pos = pos_map.get(pos)\n",
    "    if wn_pos:\n",
    "        synsets = wn.synsets(lemma, pos=wn_pos)\n",
    "        if synsets:\n",
    "            return synsets[0].name()  # Return the first synset\n",
    "    return \"UNK\"  # Unknown synset\n",
    "\n",
    "def process_corpus_incrementally(xml_path, batch_size=5000, checkpoint_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Incrementally process the XML file with </corpus> tag using batches.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    corpus_count = 0\n",
    "    batch_count = 0\n",
    "    texts, labels = [], []\n",
    "    buffer = []\n",
    "    inside_corpus = False\n",
    "\n",
    "    with open(xml_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            buffer.append(line)\n",
    "\n",
    "            # Detect the start of a corpus\n",
    "            if \"<corpus\" in line:\n",
    "                inside_corpus = True\n",
    "\n",
    "            # Detect the end of a corpus\n",
    "            if \"</corpus>\" in line and inside_corpus:\n",
    "                inside_corpus = False\n",
    "                corpus_count += 1\n",
    "                print(f\"Processing <corpus> section {corpus_count}...\")\n",
    "\n",
    "                # Process the accumulated corpus buffer\n",
    "                corpus_data = \"\".join(buffer)\n",
    "                buffer = []  # Clear buffer for the next corpus\n",
    "\n",
    "                # Use StringIO to provide a file-like object for iterparse\n",
    "                corpus_file = StringIO(corpus_data)\n",
    "                context = ET.iterparse(corpus_file, events=(\"start\", \"end\"))\n",
    "                context = iter(context)\n",
    "\n",
    "                for event, elem in context:\n",
    "                    if event == \"end\" and elem.tag == \"sentence\":\n",
    "                        sentence_text = []\n",
    "                        sentence_labels = []\n",
    "\n",
    "                        # Extract words from <wf> and <instance> elements\n",
    "                        for word_elem in elem:\n",
    "                            if word_elem.tag == \"wf\":\n",
    "                                sentence_text.append(word_elem.text)\n",
    "                            elif word_elem.tag == \"instance\":\n",
    "                                sentence_text.append(word_elem.text)\n",
    "                                synset = get_synset(word_elem.attrib.get(\"lemma\", \"\"), word_elem.attrib.get(\"pos\", \"\"))\n",
    "                                sentence_labels.append({\n",
    "                                    \"id\": word_elem.attrib.get(\"id\", \"\"),\n",
    "                                    \"lemma\": word_elem.attrib.get(\"lemma\", \"\"),\n",
    "                                    \"pos\": word_elem.attrib.get(\"pos\", \"\"),\n",
    "                                    \"synset\": synset\n",
    "                                })\n",
    "\n",
    "                        # Append extracted sentence data to lists\n",
    "                        if sentence_text:\n",
    "                            texts.append(\" \".join(sentence_text))\n",
    "                            labels.append(sentence_labels)\n",
    "\n",
    "                        # Free memory for processed elements\n",
    "                        elem.clear()\n",
    "\n",
    "                    # Save a batch when the batch size is reached\n",
    "                    if len(texts) >= batch_size:\n",
    "                        batch_count += 1\n",
    "                        checkpoint_path = os.path.join(checkpoint_dir, f\"batch_{corpus_count}_{batch_count}.csv\")\n",
    "                        save_checkpoint(texts, labels, checkpoint_path)\n",
    "                        texts, labels = [], []  # Clear batch memory\n",
    "\n",
    "                # Save remaining data in this corpus\n",
    "                if texts:\n",
    "                    batch_count += 1\n",
    "                    checkpoint_path = os.path.join(checkpoint_dir, f\"batch_{corpus_count}_{batch_count}.csv\")\n",
    "                    save_checkpoint(texts, labels, checkpoint_path)\n",
    "                    texts, labels = [], []\n",
    "\n",
    "    print(f\"Processed {corpus_count} <corpus> sections.\")\n",
    "\n",
    "def save_checkpoint(texts, labels, file_path):\n",
    "    \"\"\"\n",
    "    Save a batch of processed data to a checkpoint CSV.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"text\": texts, \"labels\": labels})\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved checkpoint to {file_path}\")\n",
    "\n",
    "def load_checkpoints(checkpoint_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Load all checkpoint files and combine them into a single DataFrame.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for file_name in sorted(os.listdir(checkpoint_dir)):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(checkpoint_dir, file_name)\n",
    "            dfs.append(pd.read_csv(file_path))\n",
    "    return pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caba18c2-12a8-493a-9338-a41decc5cd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset preview:\n",
      "                                                text  \\\n",
      "0  How long has it been since you reviewed the ob...   \n",
      "1  Have you permitted it to become a giveaway pro...   \n",
      "2  What effort do you make to assess results of y...   \n",
      "3  Do you measure its relation to reduced absente...   \n",
      "4  Have you set specific objectives for your empl...   \n",
      "\n",
      "                                              labels  \n",
      "0  [{'id': 'd000.s000.t000', 'lemma': 'long', 'po...  \n",
      "1  [{'id': 'd000.s001.t000', 'lemma': 'permit', '...  \n",
      "2  [{'id': 'd000.s002.t000', 'lemma': 'effort', '...  \n",
      "3  [{'id': 'd000.s003.t000', 'lemma': 'measure', ...  \n",
      "4  [{'id': 'd000.s004.t000', 'lemma': 'set', 'pos...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Path to the XML file\n",
    "# xml_path = \"./WSD_Training_Corpora/SemCor+OMSTI/semcor+omsti.data.xml\"\n",
    "# # Incrementally process the XML file and save checkpoints\n",
    "# process_corpus_incrementally(xml_path, batch_size=5000, checkpoint_dir=\"checkpoints\")\n",
    "\n",
    "#Combine all checkpoints into a single DataFrame\n",
    "combined_df = load_checkpoints(\"checkpoints\")\n",
    "\n",
    "# Preview the combined DataFrame\n",
    "print(\"Combined dataset preview:\")\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a95e74-b2a6-438e-9db9-25f631c6bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # To safely evaluate string representations of lists/dicts\n",
    "\n",
    "def extract_unique_synsets(combined_df):\n",
    "    \"\"\"\n",
    "    Extract unique synsets from the combined DataFrame.\n",
    "\n",
    "    Args:\n",
    "        combined_df (pd.DataFrame): The combined DataFrame with a 'labels' column.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of unique synsets.\n",
    "    \"\"\"\n",
    "    unique_synsets = set()\n",
    "\n",
    "    for labels in combined_df['labels']:\n",
    "        # Convert the string representation of labels back to a Python list\n",
    "        try:\n",
    "            label_list = ast.literal_eval(labels)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing labels: {e}\")\n",
    "            continue\n",
    "\n",
    "        for label in label_list:\n",
    "            synset = label.get('synset', 'UNK')  # Extract synset\n",
    "            if synset != 'UNK':  # Exclude unknown synsets\n",
    "                unique_synsets.add(synset)\n",
    "\n",
    "    return unique_synsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b129bed-34fb-4ad7-a35a-c826cf511107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18985\n"
     ]
    }
   ],
   "source": [
    "# Extract unique synsets from the combined DataFrame\n",
    "unique_synsets = extract_unique_synsets(combined_df)\n",
    "\n",
    "print(len(unique_synsets))\n",
    "\n",
    "synset_to_id = {synset: idx for idx, synset in enumerate(unique_synsets)}\n",
    "id_to_synset = {idx: synset for synset, idx in synset_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4518ec89-391a-4b95-9804-79b5c63549e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svaidya4/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertForTokenClassification, BertTokenizerFast\n",
    "import torch\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-large-uncased\", num_labels=len(synset_to_id))\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# Precompute embeddings for known synsets\n",
    "known_synset_embeddings = {\n",
    "    synset: model.get_input_embeddings()(\n",
    "        torch.tensor([tokenizer.convert_tokens_to_ids(synset)])\n",
    "    ).detach().numpy()\n",
    "    for synset in synset_to_id.keys()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47fb3f6a-abe9-4309-b804-1b4df4cfd61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def preprocess_dataset(data_df, tokenizer, synset_to_id, max_length=128):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by tokenizing the text and aligning labels.\n",
    "\n",
    "    Args:\n",
    "        data_df (pd.DataFrame): Parsed dataset with 'text' and 'labels'.\n",
    "        tokenizer: BERT tokenizer.\n",
    "        synset_to_id (dict): Mapping of synsets to numeric IDs.\n",
    "        max_length (int): Maximum sequence length for tokenized inputs.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: Preprocessed data with tokenized inputs and aligned labels.\n",
    "    \"\"\"\n",
    "    tokenized_data = []\n",
    "\n",
    "    for _, row in data_df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        \n",
    "        # Parse the string representation of labels into a Python list\n",
    "        try:\n",
    "            labels = ast.literal_eval(row[\"labels\"])\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing labels: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokenized_inputs = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=0)\n",
    "\n",
    "        # Map synsets to numeric IDs\n",
    "        label_dict = {\n",
    "            int(label[\"id\"].split(\".\")[-1].lstrip(\"t\")): synset_to_id.get(label[\"synset\"], -100)\n",
    "            for label in labels\n",
    "        }\n",
    "\n",
    "        # Align labels with tokens\n",
    "        token_labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:  # Special tokens\n",
    "                token_labels.append(-100)\n",
    "            elif word_id in label_dict:\n",
    "                token_labels.append(label_dict[word_id])\n",
    "            else:\n",
    "                token_labels.append(-100)\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = token_labels\n",
    "        tokenized_data.append({\n",
    "            \"input_ids\": tokenized_inputs[\"input_ids\"].squeeze().tolist(),\n",
    "            \"attention_mask\": tokenized_inputs[\"attention_mask\"].squeeze().tolist(),\n",
    "            \"labels\": token_labels\n",
    "        })\n",
    "\n",
    "    return tokenized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28084551-516b-46ad-83a0-6faa7e66f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not required, Just load the dataframe\n",
    "def convert_to_hf_dataset(tokenized_data):\n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": [data[\"input_ids\"] for data in tokenized_data],\n",
    "        \"attention_mask\": [data[\"attention_mask\"] for data in tokenized_data],\n",
    "        \"labels\": [data[\"labels\"] for data in tokenized_data],\n",
    "    })\n",
    "\n",
    "# Preprocess the parsed data\n",
    "tokenized_data = preprocess_dataset(combined_df, tokenizer, synset_to_id)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "hf_dataset = convert_to_hf_dataset(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba0103-876a-44e3-b61d-9e87fbc07268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not required\n",
    "# Save the HF Dataset\n",
    "\n",
    "hf_dataset.to_csv(\"HF_Dataframe.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95f53a80-7ac1-44bd-ac0e-11cbcd932f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2129, 2146, 2038, 2009, 2042, 2144, 2017, 8182, 1996, 11100, 1997, 2115, 5770, 1998, 2326, 2565, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 15904, 7188, 3700, 2869, 11771, 17452, 3663, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = pd.read_csv(\"HF_Dataframe.csv\")\n",
    "\n",
    "# Define patterns for cleaning\n",
    "pattern = re.compile(r\"\\s+\")  # Matches multiple spaces and \\n\n",
    "bracket_pattern = re.compile(r\"^\\[|\\]$\")  # Matches opening and closing square brackets at the start or end of a string\n",
    "\n",
    "# Function to clean spaces, newlines, and remove brackets\n",
    "def clean_and_strip(series):\n",
    "    return series.map(\n",
    "        lambda x: bracket_pattern.sub(\"\", pattern.sub(\" \", x)).strip() if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "# Function to split cleaned strings into lists of integers\n",
    "def split_to_int_list(series):\n",
    "    return series.map(\n",
    "        lambda x: [int(i) for i in x.split()] if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "# Apply cleaning to all relevant columns\n",
    "dataset[\"input_ids\"] = clean_and_strip(dataset[\"input_ids\"])\n",
    "dataset[\"attention_mask\"] = clean_and_strip(dataset[\"attention_mask\"])\n",
    "dataset[\"labels\"] = clean_and_strip(dataset[\"labels\"])\n",
    "\n",
    "# Convert cleaned strings into lists of integers\n",
    "dataset[\"input_ids\"] = split_to_int_list(dataset[\"input_ids\"])\n",
    "dataset[\"attention_mask\"] = split_to_int_list(dataset[\"attention_mask\"])\n",
    "dataset[\"labels\"] = split_to_int_list(dataset[\"labels\"])\n",
    "\n",
    "# Convert Pandas DataFrame to Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "# Display the first row of the Hugging Face Dataset\n",
    "print(hf_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55aed53a-b628-4fe9-b228-65330a3875ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 850974\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "069d951c-684b-4af3-a235-45ed68701cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Mask ignored labels\n",
    "    labels = np.array(labels)\n",
    "    valid_labels = labels != -100\n",
    "    true_labels = labels[valid_labels]\n",
    "    true_predictions = predictions[valid_labels]\n",
    "\n",
    "    # Calculate Metrics\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "    precision = precision_score(true_labels, true_predictions, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(true_labels, true_predictions, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(true_labels, true_predictions, average=\"weighted\")\n",
    "\n",
    "    # Calculate Perplexity\n",
    "    logits_tensor = torch.tensor(logits).float()  # Ensure logits are float32\n",
    "    with torch.cuda.amp.autocast(enabled=False):  # Disable mixed precision for softmax\n",
    "        probs = torch.softmax(logits_tensor, dim=-1).numpy()\n",
    "    log_probs = np.log(np.max(probs, axis=-1) + 1e-9)  # Adding epsilon for numerical stability\n",
    "    perplexity = np.exp(-np.mean(log_probs[valid_labels]))\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"perplexity\": perplexity,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbbe7a55-bb82-41b9-9380-6647cf30191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc4403fb-bdea-457d-a9ff-2f0008ca9bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc6c899-6c52-4dca-8b3b-5b4ca26eb67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly split the dataset into 70% (discarded) and 30% (used for training)\n",
    "split_dataset = hf_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "hf_dataset_split = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ca7a96b-b3a8-4a58-8255-1c81782d3922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bf420d8dd148ad8bce69e14229eb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 168067\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 2626\n",
      "  Number of trainable parameters = 353551913\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2626' max='2626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2626/2626 45:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.697800</td>\n",
       "      <td>1.253096</td>\n",
       "      <td>0.669673</td>\n",
       "      <td>0.665430</td>\n",
       "      <td>0.669673</td>\n",
       "      <td>0.644817</td>\n",
       "      <td>1.521244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2128\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./results_/checkpoint-2626\n",
      "Configuration saved in ./results_/checkpoint-2626/config.json\n",
      "Model weights saved in ./results_/checkpoint-2626/pytorch_model.bin\n",
      "tokenizer config file saved in ./results_/checkpoint-2626/tokenizer_config.json\n",
      "Special tokens file saved in ./results_/checkpoint-2626/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_/checkpoint-2626 (score: 1.2530956268310547).\n",
      "Saving model checkpoint to ./best_model_20241206_185244\n",
      "Configuration saved in ./best_model_20241206_185244/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model found! Saving to ./best_model_20241206_185244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./best_model_20241206_185244/pytorch_model.bin\n",
      "tokenizer config file saved in ./best_model_20241206_185244/tokenizer_config.json\n",
      "Special tokens file saved in ./best_model_20241206_185244/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at: ./best_model_20241206_185244\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# clear_memory()\n",
    "\n",
    "# Set up GPU for training\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Enable gradient checkpointing for memory optimization\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Move model to GPU for training\n",
    "model.to(torch.device(\"cuda\"))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_\",\n",
    "    evaluation_strategy=\"epoch\",  # Disable automatic evaluation\n",
    "    # evaluation_strategy=\"no\",  # Disable automatic evaluation\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,  # Reduce if memory issues persist\n",
    "    per_device_eval_batch_size=4,  # Smaller batch size for evaluation\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",  # Save checkpoints at the end of each epoch\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    "    optim=\"adamw_torch\",  # Optimizer for low-memory scenarios\n",
    "    gradient_accumulation_steps=8,  # Simulate larger batch sizes\n",
    "    fp16=True,  # Mixed precision training\n",
    "    load_best_model_at_end=True,  # Enable automatic loading\n",
    "    # load_best_model_at_end=False,  # Disable automatic loading\n",
    "    eval_accumulation_steps = 50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_val_split = hf_dataset_split.train_test_split(test_size=0.0125, seed=42)\n",
    "train_dataset = train_val_split[\"train\"]\n",
    "val_dataset = train_val_split[\"test\"]\n",
    "\n",
    "# Move validation dataset to CPU\n",
    "val_dataset = val_dataset.map(lambda x: {k: torch.tensor(v).to(\"cpu\") for k, v in x.items()})\n",
    "\n",
    "# from transformers import BertForTokenClassification, BertTokenizerFast\n",
    "\n",
    "# Load the model\n",
    "model_path = \"./best_model_20241206_165542\"\n",
    "model = BertForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=None,  # No automatic evaluation during training\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,  # Add custom metrics\n",
    ")\n",
    "\n",
    "best_metric = float(\"inf\")  # Assuming lower metric is better (e.g., loss)\n",
    "\n",
    "model.to(torch.device(\"cuda\"))\n",
    "clear_memory()\n",
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=\"./results_/checkpoint-1323\")\n",
    "\n",
    "current_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "best_model_path = f\"./best_model_{current_timestamp}\"\n",
    "print(f\"New best model found! Saving to {best_model_path}\")\n",
    "trainer.save_model(best_model_path)\n",
    "\n",
    "# Final best model path\n",
    "print(f\"Best model saved at: {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "919d12c8-d5fc-4085-9728-95cd9d88aa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80cf44ad21c4ed0bd0db5ec916ea6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 169982\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 2656\n",
      "  Number of trainable parameters = 353551913\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2656' max='2656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2656/2656 56:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.735200</td>\n",
       "      <td>1.317335</td>\n",
       "      <td>0.676806</td>\n",
       "      <td>0.645754</td>\n",
       "      <td>0.676806</td>\n",
       "      <td>0.648687</td>\n",
       "      <td>1.335419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 213\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./results/checkpoint-2656\n",
      "Configuration saved in ./results/checkpoint-2656/config.json\n",
      "Model weights saved in ./results/checkpoint-2656/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2656/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2656/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-2656 (score: 1.3173346519470215).\n",
      "Saving model checkpoint to ./best_model_20241206_194927\n",
      "Configuration saved in ./best_model_20241206_194927/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model found! Saving to ./best_model_20241206_194927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./best_model_20241206_194927/pytorch_model.bin\n",
      "tokenizer config file saved in ./best_model_20241206_194927/tokenizer_config.json\n",
      "Special tokens file saved in ./best_model_20241206_194927/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at: ./best_model_20241206_194927\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# clear_memory()\n",
    "\n",
    "# Set up GPU for training\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Enable gradient checkpointing for memory optimization\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Move model to GPU for training\n",
    "model.to(torch.device(\"cuda\"))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Disable automatic evaluation\n",
    "    # evaluation_strategy=\"no\",  # Disable automatic evaluation\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,  # Reduce if memory issues persist\n",
    "    per_device_eval_batch_size=4,  # Smaller batch size for evaluation\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",  # Save checkpoints at the end of each epoch\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    "    optim=\"adamw_torch\",  # Optimizer for low-memory scenarios\n",
    "    gradient_accumulation_steps=8,  # Simulate larger batch sizes\n",
    "    fp16=True,  # Mixed precision training\n",
    "    load_best_model_at_end=True,  # Enable automatic loading\n",
    "    # load_best_model_at_end=False,  # Disable automatic loading\n",
    "    eval_accumulation_steps = 50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_val_split = hf_dataset_split.train_test_split(test_size=0.00125, seed=42)\n",
    "train_dataset = train_val_split[\"train\"]\n",
    "val_dataset = train_val_split[\"test\"]\n",
    "\n",
    "# Move validation dataset to CPU\n",
    "val_dataset = val_dataset.map(lambda x: {k: torch.tensor(v).to(\"cpu\") for k, v in x.items()})\n",
    "\n",
    "# from transformers import BertForTokenClassification, BertTokenizerFast\n",
    "\n",
    "# # Load the model\n",
    "# model_path = \"./best_model_20241204_114605\"\n",
    "# model = BertForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=None,  # No automatic evaluation during training\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,  # Add custom metrics\n",
    ")\n",
    "\n",
    "best_metric = float(\"inf\")  # Assuming lower metric is better (e.g., loss)\n",
    "\n",
    "model.to(torch.device(\"cuda\"))\n",
    "clear_memory()\n",
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=\"./results/checkpoint-11030\")\n",
    "\n",
    "current_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "best_model_path = f\"./best_model_{current_timestamp}\"\n",
    "print(f\"New best model found! Saving to {best_model_path}\")\n",
    "trainer.save_model(best_model_path)\n",
    "\n",
    "# Final best model path\n",
    "print(f\"Best model saved at: {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd34fcc-7bc7-4fde-aba9-76bca26bf116",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Only run this if not training the model. Used to load it\n",
    "\n",
    "model_path = \"./best_model_20241206_194927\"\n",
    "model = BertForTokenClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,  # Add custom metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55d77436-2ee8-4701-9f94-3453157abe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_synset_details(word, synset_name):\n",
    "    \"\"\"\n",
    "    Fetch details of the synset: definition and examples.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to find the synset for.\n",
    "        synset_name (str): The WordNet synset name (e.g., 'dog.n.01').\n",
    "\n",
    "    Returns:\n",
    "        dict: Synset details including definition and examples.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        synset = wn.synset(synset_name)\n",
    "        return {\n",
    "            \"word\": word,\n",
    "            \"definition\": synset.definition(),\n",
    "            \"examples\": synset.examples()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching synset details: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da9a00f6-8a91-46f7-b79f-97fc88990ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "\n",
    "def predict_synsets(sentence, model, tokenizer, id_to_synset, max_length=128):\n",
    "    \"\"\"\n",
    "    Predict WordNet synsets for each word in a given sentence using the fine-tuned model.\n",
    "    \"\"\"\n",
    "    # Determine the device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move model to the device\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    inputs = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        is_split_into_words=False\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the device\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract logits and predictions\n",
    "    logits = outputs.logits\n",
    "    predictions = logits.argmax(dim=-1).squeeze().tolist()\n",
    "\n",
    "    # Map predictions to WordNet synsets\n",
    "    input_ids = inputs['input_ids'].squeeze().tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    word_ids = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Handle wordpiece alignment\n",
    "    predicted_synsets = []\n",
    "    for token_id, pred in zip(word_ids, predictions):\n",
    "        if pred in id_to_synset:\n",
    "            predicted_synsets.append(id_to_synset[pred])\n",
    "        else:\n",
    "            predicted_synsets.append(\"UNK\")\n",
    "\n",
    "    # Aggregate results by word\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "    aggregated_results = []\n",
    "    for token, synset in zip(tokenized_sentence, predicted_synsets):\n",
    "        aggregated_results.append((token, synset))\n",
    "\n",
    "    return aggregated_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f092ea0d-e031-4d5a-86ab-f0c2116e106d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Synsets:\n",
      "everything: liquid.a.01\n",
      "happens: praise.n.01\n",
      "for: liquid.a.01\n",
      "its: liquid.a.01\n",
      "own: liquid.a.01\n",
      "good: liquid.a.01\n",
      ".: liquid.a.01\n",
      "[('everything', 'liquid.a.01'), ('happens', 'praise.n.01'), ('for', 'liquid.a.01'), ('its', 'liquid.a.01'), ('own', 'liquid.a.01'), ('good', 'liquid.a.01'), ('.', 'liquid.a.01')]\n",
      "\n",
      "Predicted Synsets and Definitions:\n",
      "{'word': 'own', 'definition': 'existing as or having characteristics of a liquid; especially tending to flow', 'examples': ['water and milk and blood are liquid substances']}\n"
     ]
    }
   ],
   "source": [
    "# sentence = input(\"Enter a sentence: Everything happens for its own good.\")\n",
    "# sentence = \"I was delighted to find money in the bank\"\n",
    "sentence = \"Everything happens for its own good.\"\n",
    "\n",
    "# sentence = \"We saw ducks near the bank\"\n",
    "\n",
    "\n",
    "word = \"own\"\n",
    "\n",
    "\n",
    "# Predict synsets\n",
    "predictions = predict_synsets(sentence, model, tokenizer, id_to_synset)\n",
    "\n",
    "\n",
    "# Display results|\n",
    "print(\"\\nPredicted Synsets:\")\n",
    "for token, synset in predictions:\n",
    "    print(f\"{token}: {synset}\")\n",
    "\n",
    "print(predictions)\n",
    "    \n",
    "print(\"\\nPredicted Synsets and Definitions:\")\n",
    "for token, synset_name in predictions:\n",
    "    if(token == word):\n",
    "        definition = get_synset_details(token, synset_name)\n",
    "        print(definition)\n",
    "    # else:\n",
    "    #     print(\"No synset found\")\n",
    "    # if synset_name != \"UNK\":  # If the synset is valid\n",
    "    #     definition = get_synset_details(token, synset_name)\n",
    "    #     print(f\"{token}: {synset_name} - {definition}\")\n",
    "    # else:\n",
    "    #     print(f\"{token}: {synset_name} - No synset found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26db92b-e329-4ec5-9396-6c4bf7743fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
