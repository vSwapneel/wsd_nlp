{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install datasets\n",
    "# %pip install transformers\n",
    "# %pip install torch torchvision\n",
    "# %pip install bitsandbytes\n",
    "# %pip install transformers[torch]\n",
    "# %pip install 'accelerate>={ACCELERATE_MIN_VERSION}'\n",
    "# %pip install transformers torch accelerate\n",
    "# %pip install -U accelerate\n",
    "# %pip install --upgrade pip\n",
    "# %pip uninstall transformers accelerate -y\n",
    "# %pip install transformers[torch] accelerate\n",
    "# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "# %pip install --upgrade accelerate\n",
    "# %pip uninstall transformers accelerate torch torchvision -y\n",
    "# %pip install transformers[torch] torch accelerate\n",
    "# %pip install transformers==4.26.0 accelerate==0.26.0 torch==1.12.0 torchvision\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# %pip install torch torchvision torchaudio\n",
    "# %pip install transformers\n",
    "# %pip uninstall torch torchvision torchaudio -y\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1Xz6iWZwaUR",
    "outputId": "3805bf78-e5d4-4bc5-b3e3-a494b567cb31"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrgE7McbtbQ_",
    "outputId": "fb148cd5-a679-4644-ef2e-0a86da28bbba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/svaidya4/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/svaidya4/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S8z3lsqBJihF"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "def get_synset(lemma, pos, model, tokenizer, known_synset_embeddings, collect_only=False, unique_synsets=None):\n",
    "    \"\"\"\n",
    "    Map a lemma and POS to its corresponding WordNet synset.\n",
    "    Use embedding-based similarity fallback if necessary.\n",
    "\n",
    "    Args:\n",
    "        lemma (str): Lemma of the word.\n",
    "        pos (str): Part of speech of the word.\n",
    "        model: Fine-tuned BERT model.\n",
    "        tokenizer: Tokenizer used with the model.\n",
    "        known_synset_embeddings: Precomputed embeddings for known synsets.\n",
    "        collect_only (bool): If True, only collect synsets without fallback.\n",
    "        unique_synsets (set): A set to collect unique synsets.\n",
    "\n",
    "    Returns:\n",
    "        str: Synset name or \"UNK\" if not found.\n",
    "    \"\"\"\n",
    "    pos_map = {\"NOUN\": wn.NOUN, \"VERB\": wn.VERB, \"ADJ\": wn.ADJ, \"ADV\": wn.ADV}\n",
    "    wn_pos = pos_map.get(pos)\n",
    "\n",
    "    # Attempt to retrieve synsets from WordNet\n",
    "    if wn_pos:\n",
    "        synsets = wn.synsets(lemma, pos=wn_pos)\n",
    "        if synsets:\n",
    "            synset = synsets[0].name()  # First synset\n",
    "            if collect_only and unique_synsets is not None:\n",
    "                unique_synsets.add(synset)  # Collect unique synsets\n",
    "            return synset\n",
    "\n",
    "    # Skip fallback during collection\n",
    "    if collect_only:\n",
    "        return \"UNK\"\n",
    "\n",
    "    # Fallback: Find closest synset embedding for OOV words\n",
    "    try:\n",
    "        word_embedding = model.get_input_embeddings()(\n",
    "            torch.tensor([tokenizer.convert_tokens_to_ids(lemma)])\n",
    "        ).detach().numpy()\n",
    "        closest_synset = get_closest_synset(word_embedding, known_synset_embeddings)\n",
    "        return closest_synset\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding closest synset for OOV word '{lemma}': {e}\")\n",
    "        return \"UNK\"\n",
    "\n",
    "def get_closest_synset(embedding, known_synset_embeddings):\n",
    "    \"\"\"\n",
    "    Find the closest synset based on cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        embedding (np.ndarray): Embedding for the OOV word.\n",
    "        known_synset_embeddings (dict): Precomputed embeddings for known synsets.\n",
    "\n",
    "    Returns:\n",
    "        str: Synset with the highest similarity.\n",
    "    \"\"\"\n",
    "    similarities = {\n",
    "        synset: cosine_similarity(embedding.reshape(1, -1), known_embedding.reshape(1, -1))[0][0]\n",
    "        for synset, known_embedding in known_synset_embeddings.items()\n",
    "    }\n",
    "    return max(similarities, key=similarities.get)\n",
    "\n",
    "def parse_xml_dataset(xml_path, model=None, tokenizer=None, known_synset_embeddings=None, collect_only=False):\n",
    "    \"\"\"\n",
    "    Parse XML dataset, collect unique synsets, and preprocess sentences in one pass.\n",
    "\n",
    "    Args:\n",
    "        xml_path (str): Path to the XML file.\n",
    "        model: Fine-tuned BERT model (optional).\n",
    "        tokenizer: Tokenizer used with the model (optional).\n",
    "        known_synset_embeddings: Precomputed embeddings for known synsets (optional).\n",
    "        collect_only (bool): If True, only collect unique synsets without preprocessing.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing texts and labels.\n",
    "        set: Unique synsets collected from the dataset.\n",
    "    \"\"\"\n",
    "    texts, labels = [], []\n",
    "    unique_synsets = set()\n",
    "\n",
    "    for event, elem in ET.iterparse(xml_path, events=(\"start\", \"end\")):\n",
    "        if event == \"end\" and elem.tag == \"sentence\":\n",
    "            sentence_text = []\n",
    "            sentence_labels = []\n",
    "\n",
    "            for child in elem:\n",
    "                if child.tag == \"wf\":\n",
    "                    sentence_text.append(child.text)\n",
    "                elif child.tag == \"instance\":\n",
    "                    sentence_text.append(child.text)\n",
    "                    synset = get_synset(\n",
    "                        child.attrib.get(\"lemma\", \"\"),\n",
    "                        child.attrib.get(\"pos\", \"\"),\n",
    "                        model,\n",
    "                        tokenizer,\n",
    "                        known_synset_embeddings,\n",
    "                        collect_only=collect_only,\n",
    "                        unique_synsets=unique_synsets\n",
    "                    )\n",
    "                    sentence_labels.append({\n",
    "                        \"id\": child.attrib.get(\"id\", \"\"),\n",
    "                        \"lemma\": child.attrib.get(\"lemma\", \"\"),\n",
    "                        \"pos\": child.attrib.get(\"pos\", \"\"),\n",
    "                        \"synset\": synset\n",
    "                    })\n",
    "\n",
    "            texts.append(\" \".join(sentence_text))\n",
    "            labels.append(sentence_labels)\n",
    "            elem.clear()  # Free memory\n",
    "\n",
    "    return pd.DataFrame({\"text\": texts, \"labels\": labels}), unique_synsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cCX7LHhjs-ih"
   },
   "outputs": [],
   "source": [
    "xml_path = \"./WSD_Training_Corpora/SemCor/semcor.data.xml\"\n",
    "\n",
    "# Step 1: Collect synsets and preprocess dataset\n",
    "parsed_data, unique_synsets = parse_xml_dataset(xml_path, collect_only=True)\n",
    "\n",
    "\n",
    "# Step 2: Map synsets to IDs\n",
    "synset_to_id = {synset: idx for idx, synset in enumerate(unique_synsets)}\n",
    "id_to_synset = {idx: synset for synset, idx in synset_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336,
     "referenced_widgets": [
      "c05bcb16a4e74722a18f68407b9f9e79",
      "3528559911c6447e8145c4f5da80f15a",
      "3ad62049d0514655912c801a3a6c33af",
      "a9fe243d8cfe48a7900c9b8784f3f951",
      "e67f94e0b1fe4cdf9be788540ba5b921",
      "0bc8a0e16cd64bcea716208adf605475",
      "7072ae437f054c679f1f638ddc8a5fcf",
      "06a444a0f4994ee3b678e6d38e74d9f2",
      "f7c4addd8e2146b4b45e035c693cc558",
      "ed57552d689b405b9dc63eb2ebf89628",
      "bfe550841e0340efa726623d13d56d42",
      "77b2a19e25a24d90ad73a865505d68a8",
      "59d0d5d8e2e24b32a51719a23b450c79",
      "8352f29cbe2a42069fd1c2491e54355c",
      "837cfc32f9a34ada8c810fe3ba9b889b",
      "687c1412605b456e81b657a66de40d68",
      "a88d0525d04e418498a95932e6cb9bc7",
      "6609b1ff7b054fadbb1a2f7039cab62a",
      "866d39cfef2243cf8921df0b6c056ed6",
      "84c0c32bf99045fd9a23bfcb55189242",
      "dc246ef0646b4c9eaa44b627eb35557c",
      "dc22f864fc8f4ec3a63d573ee2d785c1",
      "29c0a18d551d437ba46ad56f121ccf0a",
      "6b68842c581f4027a61756cd1712ccc5",
      "c72903775d144d0e8531bc5206346fe5",
      "61772e9e02874be48d30d6e2706c203a",
      "2bb89fe85cdc420ba1622b5fcf297ddc",
      "961c3571d38e4c13a76d6246470f9497",
      "4a2829465b004a1496976ce2cdf30c26",
      "a0ff159a3dc548c5881db5b772f0c6e2",
      "258c4abcb7614310b46626b3ad75f26c",
      "0a995c7a035d441ea370cebe72eaf8f0",
      "e53607d484084e1ca67d5b2197900e75",
      "9bc755cbdd6745d3a872a07229f913fb",
      "cc5116f44b6249d2baa9c72fa1850f0c",
      "13c55af7d4aa41749834e95f24c847d6",
      "833cf2599d574c79815f1abd963d2b4a",
      "9b39f39e51cc43a4a85c09f02bef2baa",
      "1fb34004a1304c3f99053b68e0aa978e",
      "fdfac33240d04a9b8ae1f279ef3f9e1e",
      "2726deb1ca1d4676986b1c69b3f52d74",
      "ed87a50374dc4cf3906ba1bdffcab1cc",
      "867c79cb14b24ad4a9001f480c03ceae",
      "a519dd3bafa941ea8c096dd957361baf",
      "419a7943ed37457c970370a10a84d916",
      "113a61d6c9d24dd5bb1ad7c68b60d340",
      "f5bc40fe5dd8412cab460d1feac02397",
      "52b8ccf4212348f2a3cd7b4f049b5d6e",
      "9831527b26bd4cd488155a98aa4ec661",
      "09d7738d0e7449ee88c6d14a71ba047f",
      "21c5b56432c145749e7f0f5b3664aee6",
      "dbd76c78abf64bd7be9161ed777972bb",
      "46929edf6ad54411a531ee7c60973bc0",
      "922e5c71c5914e8eafefbd23c5c0dfe1",
      "2dcf4b880d984be8aa26095f45efb796"
     ]
    },
    "id": "z-SO6iVwtOim",
    "outputId": "9377a7df-7e4a-49b9-e935-f44193014c68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svaidya4/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at kanishka/GlossBERT were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizerFast, AutoModel\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-large-uncased\", num_labels=len(synset_to_id))\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# Load model directly\n",
    "model = AutoModel.from_pretrained(\"kanishka/GlossBERT\")\n",
    "\n",
    "# Precompute embeddings for known synsets\n",
    "known_synset_embeddings = {\n",
    "    synset: model.get_input_embeddings()(\n",
    "        torch.tensor([tokenizer.convert_tokens_to_ids(synset)])\n",
    "    ).detach().numpy()\n",
    "    for synset in synset_to_id.keys()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yXdCSMkntRcU"
   },
   "outputs": [],
   "source": [
    "# Parse dataset with embedding-based fallback\n",
    "final_data, _ = parse_xml_dataset(xml_path, model, tokenizer, known_synset_embeddings, collect_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "k_AExkwuuwGp"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(data_df, tokenizer, synset_to_id, max_length=128):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by tokenizing the text and aligning labels.\n",
    "\n",
    "    Args:\n",
    "        data_df (pd.DataFrame): Parsed dataset with 'text' and 'labels'.\n",
    "        tokenizer: BERT tokenizer.\n",
    "        synset_to_id (dict): Mapping of synsets to numeric IDs.\n",
    "        max_length (int): Maximum sequence length for tokenized inputs.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: Preprocessed data with tokenized inputs and aligned labels.\n",
    "    \"\"\"\n",
    "    tokenized_data = []\n",
    "\n",
    "    for _, row in data_df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        labels = row[\"labels\"]\n",
    "\n",
    "        # Tokenize the text\n",
    "        tokenized_inputs = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=0)\n",
    "\n",
    "        # Map synsets to numeric IDs\n",
    "        label_dict = {\n",
    "            int(label[\"id\"].split(\".\")[-1].lstrip(\"t\")): synset_to_id.get(label[\"synset\"], -100)\n",
    "            for label in labels\n",
    "        }\n",
    "\n",
    "        # Align labels with tokens\n",
    "        token_labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:  # Special tokens\n",
    "                token_labels.append(-100)\n",
    "            elif word_id in label_dict:\n",
    "                token_labels.append(label_dict[word_id])\n",
    "            else:\n",
    "                token_labels.append(-100)\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = token_labels\n",
    "        tokenized_data.append({\n",
    "            \"input_ids\": tokenized_inputs[\"input_ids\"].squeeze().tolist(),\n",
    "            \"attention_mask\": tokenized_inputs[\"attention_mask\"].squeeze().tolist(),\n",
    "            \"labels\": token_labels\n",
    "        })\n",
    "\n",
    "    return tokenized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0tnd2XEGv5Xj"
   },
   "outputs": [],
   "source": [
    "def convert_to_hf_dataset(tokenized_data):\n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": [data[\"input_ids\"] for data in tokenized_data],\n",
    "        \"attention_mask\": [data[\"attention_mask\"] for data in tokenized_data],\n",
    "        \"labels\": [data[\"labels\"] for data in tokenized_data],\n",
    "    })\n",
    "\n",
    "# Preprocess the parsed data\n",
    "tokenized_data = preprocess_dataset(final_data, tokenizer, synset_to_id)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "hf_dataset = convert_to_hf_dataset(tokenized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49BopCaExVeG",
    "outputId": "7007e7fe-806e-48f8-b9b6-c6a14c89e383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 37176\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Q2WSd2K7xiKZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     \"\"\"\n",
    "#     Compute evaluation metrics: Precision, Recall, F1-Score, and Perplexity.\n",
    "\n",
    "#     Args:\n",
    "#         eval_pred: A tuple of (logits, labels) from the Trainer evaluation.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: Dictionary of computed metrics.\n",
    "#     \"\"\"\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "#     # Mask out padding and special tokens (-100)\n",
    "#     true_labels = labels[labels != -100]\n",
    "#     true_predictions = predictions[labels != -100]\n",
    "\n",
    "#     # Calculate Precision, Recall, and F1-Score\n",
    "#     precision = precision_score(true_labels, true_predictions, average=\"weighted\")\n",
    "#     recall = recall_score(true_labels, true_predictions, average=\"weighted\")\n",
    "#     f1 = f1_score(true_labels, true_predictions, average=\"weighted\")\n",
    "\n",
    "#     # Calculate Perplexity\n",
    "#     probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "#     log_probs = np.log(np.max(probs, axis=-1) + 1e-9)  # Adding epsilon for numerical stability\n",
    "#     perplexity = np.exp(-np.mean(log_probs[labels != -100]))\n",
    "\n",
    "#     return {\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall,\n",
    "#         \"f1\": f1,\n",
    "#         \"perplexity\": perplexity,\n",
    "#     }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Mask ignored labels\n",
    "    labels = np.array(labels)\n",
    "    valid_labels = labels != -100\n",
    "    true_labels = labels[valid_labels]\n",
    "    true_predictions = predictions[valid_labels]\n",
    "\n",
    "    # Calculate Metrics\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "    precision = precision_score(true_labels, true_predictions, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(true_labels, true_predictions, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(true_labels, true_predictions, average=\"weighted\")\n",
    "\n",
    "    # Calculate Perplexity\n",
    "    logits_tensor = torch.tensor(logits).float()  # Ensure logits are float32\n",
    "    with torch.cuda.amp.autocast(enabled=False):  # Disable mixed precision for softmax\n",
    "        probs = torch.softmax(logits_tensor, dim=-1).numpy()\n",
    "    log_probs = np.log(np.max(probs, axis=-1) + 1e-9)  # Adding epsilon for numerical stability\n",
    "    perplexity = np.exp(-np.mean(log_probs[valid_labels]))\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"perplexity\": perplexity,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "id": "QWfumZrGyE0s",
    "outputId": "ab9c78d7-a47b-4871-8403-cafe7246fd2f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4e717828bd4ab49474efd8ad9bde82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 35317\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1103\n",
      "  Number of trainable parameters = 353550888\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1103' max='1103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1103/1103 08:07, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.499700</td>\n",
       "      <td>5.492875</td>\n",
       "      <td>0.432120</td>\n",
       "      <td>0.367997</td>\n",
       "      <td>0.432120</td>\n",
       "      <td>0.385717</td>\n",
       "      <td>7.721401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./results/checkpoint-1103\n",
      "Configuration saved in ./results/checkpoint-1103/config.json\n",
      "Model weights saved in ./results/checkpoint-1103/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1103/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1103/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-1103 (score: 5.492874622344971).\n",
      "Saving model checkpoint to ./best_model_20241207_000429\n",
      "Configuration saved in ./best_model_20241207_000429/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model found! Saving to ./best_model_20241207_000429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./best_model_20241207_000429/pytorch_model.bin\n",
      "tokenizer config file saved in ./best_model_20241207_000429/tokenizer_config.json\n",
      "Special tokens file saved in ./best_model_20241207_000429/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at: ./best_model_20241207_000429\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Memory management utility\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# Set up GPU for training\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Enable gradient checkpointing for memory optimization\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Move model to GPU for training\n",
    "model.to(torch.device(\"cuda\"))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Disable automatic evaluation\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,  # Reduce if memory issues persist\n",
    "    per_device_eval_batch_size=4,  # Smaller batch size for evaluation\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",  # Save checkpoints at the end of each epoch\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    "    optim=\"adamw_torch\",  # Optimizer for low-memory scenarios\n",
    "    gradient_accumulation_steps=4,  # Simulate larger batch sizes\n",
    "    fp16=True,  # Mixed precision training\n",
    "    load_best_model_at_end=True,  # Enable automatic loading\n",
    "    # load_best_model_at_end=False,  # Disable automatic loading\n",
    "    eval_accumulation_steps = 50,\n",
    ")\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_val_split = hf_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = train_val_split[\"train\"]\n",
    "val_dataset = train_val_split[\"test\"]\n",
    "\n",
    "# Move validation dataset to CPU\n",
    "val_dataset = val_dataset.map(lambda x: {k: torch.tensor(v).to(\"cpu\") for k, v in x.items()})\n",
    "\n",
    "from transformers import BertForTokenClassification, BertTokenizerFast\n",
    "\n",
    "# Load the model\n",
    "model_path = \"./best_model_20241202_220941\"\n",
    "model = BertForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=None,  # No automatic evaluation during training\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,  # Add custom metrics\n",
    ")\n",
    "\n",
    "# best_model_path = None\n",
    "best_metric = float(\"inf\")  # Assuming lower metric is better (e.g., loss)\n",
    "\n",
    "model.to(torch.device(\"cuda\"))\n",
    "clear_memory()\n",
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=\"./results/checkpoint-11030\")\n",
    "\n",
    "current_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "best_model_path = f\"./best_model_{current_timestamp}\"\n",
    "print(f\"New best model found! Saving to {best_model_path}\")\n",
    "trainer.save_model(best_model_path)\n",
    "\n",
    "# Final best model path\n",
    "print(f\"Best model saved at: {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afc90a78bde46e09e09ae985e6a0447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:593: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "***** Running training *****\n",
      "  Num examples = 35317\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 16545\n",
      "  Number of trainable parameters = 353550888\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16545' max='16545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16545/16545 2:20:51, Epoch 14/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.122400</td>\n",
       "      <td>4.865142</td>\n",
       "      <td>0.248260</td>\n",
       "      <td>0.357270</td>\n",
       "      <td>0.277423</td>\n",
       "      <td>4.695312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.765900</td>\n",
       "      <td>4.610922</td>\n",
       "      <td>0.276630</td>\n",
       "      <td>0.384134</td>\n",
       "      <td>0.306743</td>\n",
       "      <td>4.472656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.368500</td>\n",
       "      <td>4.515942</td>\n",
       "      <td>0.300286</td>\n",
       "      <td>0.401538</td>\n",
       "      <td>0.328368</td>\n",
       "      <td>3.800781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.102700</td>\n",
       "      <td>4.342058</td>\n",
       "      <td>0.322309</td>\n",
       "      <td>0.420123</td>\n",
       "      <td>0.350135</td>\n",
       "      <td>3.525391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.886500</td>\n",
       "      <td>4.252907</td>\n",
       "      <td>0.347693</td>\n",
       "      <td>0.435583</td>\n",
       "      <td>0.371648</td>\n",
       "      <td>3.337891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.586200</td>\n",
       "      <td>4.221997</td>\n",
       "      <td>0.358819</td>\n",
       "      <td>0.441159</td>\n",
       "      <td>0.380455</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.380800</td>\n",
       "      <td>4.222005</td>\n",
       "      <td>0.375421</td>\n",
       "      <td>0.454169</td>\n",
       "      <td>0.395762</td>\n",
       "      <td>2.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.283600</td>\n",
       "      <td>4.102341</td>\n",
       "      <td>0.396843</td>\n",
       "      <td>0.469629</td>\n",
       "      <td>0.413960</td>\n",
       "      <td>2.853516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.089400</td>\n",
       "      <td>4.115480</td>\n",
       "      <td>0.396284</td>\n",
       "      <td>0.469460</td>\n",
       "      <td>0.414239</td>\n",
       "      <td>2.650391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.972800</td>\n",
       "      <td>4.114380</td>\n",
       "      <td>0.423233</td>\n",
       "      <td>0.489651</td>\n",
       "      <td>0.438110</td>\n",
       "      <td>2.574219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.854500</td>\n",
       "      <td>4.078634</td>\n",
       "      <td>0.421881</td>\n",
       "      <td>0.487370</td>\n",
       "      <td>0.437081</td>\n",
       "      <td>2.523438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.838600</td>\n",
       "      <td>4.090305</td>\n",
       "      <td>0.429313</td>\n",
       "      <td>0.492101</td>\n",
       "      <td>0.442759</td>\n",
       "      <td>2.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.742100</td>\n",
       "      <td>4.107839</td>\n",
       "      <td>0.429275</td>\n",
       "      <td>0.488553</td>\n",
       "      <td>0.441680</td>\n",
       "      <td>2.423828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.697000</td>\n",
       "      <td>4.049660</td>\n",
       "      <td>0.438570</td>\n",
       "      <td>0.499366</td>\n",
       "      <td>0.452016</td>\n",
       "      <td>2.419922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.639900</td>\n",
       "      <td>4.047450</td>\n",
       "      <td>0.442783</td>\n",
       "      <td>0.503337</td>\n",
       "      <td>0.456183</td>\n",
       "      <td>2.410156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-1103\n",
      "Configuration saved in ./results/checkpoint-1103/config.json\n",
      "Model weights saved in ./results/checkpoint-1103/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1103/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1103/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-2206\n",
      "Configuration saved in ./results/checkpoint-2206/config.json\n",
      "Model weights saved in ./results/checkpoint-2206/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2206/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2206/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-3309\n",
      "Configuration saved in ./results/checkpoint-3309/config.json\n",
      "Model weights saved in ./results/checkpoint-3309/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3309/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3309/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-4412\n",
      "Configuration saved in ./results/checkpoint-4412/config.json\n",
      "Model weights saved in ./results/checkpoint-4412/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4412/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4412/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-5515\n",
      "Configuration saved in ./results/checkpoint-5515/config.json\n",
      "Model weights saved in ./results/checkpoint-5515/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5515/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5515/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-6618\n",
      "Configuration saved in ./results/checkpoint-6618/config.json\n",
      "Model weights saved in ./results/checkpoint-6618/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-6618/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6618/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-7721\n",
      "Configuration saved in ./results/checkpoint-7721/config.json\n",
      "Model weights saved in ./results/checkpoint-7721/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-7721/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-7721/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-8824\n",
      "Configuration saved in ./results/checkpoint-8824/config.json\n",
      "Model weights saved in ./results/checkpoint-8824/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-8824/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-8824/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-9927\n",
      "Configuration saved in ./results/checkpoint-9927/config.json\n",
      "Model weights saved in ./results/checkpoint-9927/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-9927/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-9927/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-11030\n",
      "Configuration saved in ./results/checkpoint-11030/config.json\n",
      "Model weights saved in ./results/checkpoint-11030/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-11030/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-11030/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-12133\n",
      "Configuration saved in ./results/checkpoint-12133/config.json\n",
      "Model weights saved in ./results/checkpoint-12133/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-12133/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-12133/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-13236\n",
      "Configuration saved in ./results/checkpoint-13236/config.json\n",
      "Model weights saved in ./results/checkpoint-13236/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-13236/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-13236/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-14339\n",
      "Configuration saved in ./results/checkpoint-14339/config.json\n",
      "Model weights saved in ./results/checkpoint-14339/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-14339/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-14339/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-15442\n",
      "Configuration saved in ./results/checkpoint-15442/config.json\n",
      "Model weights saved in ./results/checkpoint-15442/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-15442/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-15442/special_tokens_map.json\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./results/checkpoint-16545\n",
      "Configuration saved in ./results/checkpoint-16545/config.json\n",
      "Model weights saved in ./results/checkpoint-16545/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-16545/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-16545/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-16545 (score: 4.047449588775635).\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2066: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16545, training_loss=2.5204320230308204, metrics={'train_runtime': 8451.9391, 'train_samples_per_second': 62.679, 'train_steps_per_second': 1.958, 'total_flos': 1.3090753681429709e+17, 'train_loss': 2.5204320230308204, 'epoch': 15.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Memory management utility\n",
    "def clear_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# Set up GPU for training\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Enable gradient checkpointing for memory optimization\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Move model to GPU for training\n",
    "model.to(torch.device(\"cuda\"))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Disable automatic evaluation\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,  # Reduce if memory issues persist\n",
    "    per_device_eval_batch_size=4,  # Smaller batch size for evaluation\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",  # Save checkpoints at the end of each epoch\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    "    optim=\"adamw_torch\",  # Optimizer for low-memory scenarios\n",
    "    gradient_accumulation_steps=4,  # Simulate larger batch sizes\n",
    "    fp16=True,  # Mixed precision training\n",
    "    load_best_model_at_end=True,  # Enable automatic loading\n",
    "    # load_best_model_at_end=False,  # Disable automatic loading\n",
    "    eval_accumulation_steps = 50,\n",
    ")\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_val_split = hf_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = train_val_split[\"train\"]\n",
    "val_dataset = train_val_split[\"test\"]\n",
    "\n",
    "# Move validation dataset to CPU\n",
    "val_dataset = val_dataset.map(lambda x: {k: torch.tensor(v).to(\"cpu\") for k, v in x.items()})\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=None,  # No automatic evaluation during training\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,  # Add custom metrics\n",
    ")\n",
    "\n",
    "# best_model_path = None\n",
    "best_metric = float(\"inf\")  # Assuming lower metric is better (e.g., loss)\n",
    "\n",
    "model.to(torch.device(\"cuda\"))\n",
    "clear_memory()\n",
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=\"./results/checkpoint-11030\")\n",
    "\n",
    "# current_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# best_model_path = f\"./best_model_{current_timestamp}\"\n",
    "# print(f\"New best model found! Saving to {best_model_path}\")\n",
    "# trainer.save_model(best_model_path)\n",
    "\n",
    "# # Final best model path\n",
    "# print(f\"Best model saved at: {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1859\n",
      "  Batch size = 4\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='465' max='465' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [465/465 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 14.762930870056152, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_perplexity': 2.052776336669922, 'eval_runtime': 35.2721, 'eval_samples_per_second': 52.705, 'eval_steps_per_second': 13.183, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate(val_dataset)\n",
    "print(f\"Evaluation Results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 558\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating chunk 1/1 (Samples 0-558)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='558' max='558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [558/558 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear CUDA memory to prevent memory overflow.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA memory cleared.\")\n",
    "\n",
    "# Set evaluation batch size and accumulation steps\n",
    "training_args.per_device_eval_batch_size = 1\n",
    "training_args.eval_accumulation_steps = 10\n",
    "torch.cuda.empty_cache()  # Clear memory\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1000\n",
    "num_chunks = (len(val_dataset) + chunk_size - 1) // chunk_size  # Total number of chunks\n",
    "\n",
    "# Accumulate metrics\n",
    "all_metrics = []\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(val_dataset))\n",
    "    \n",
    "    # Select a chunk of the dataset\n",
    "    val_dataset_chunk = val_dataset.select(range(start_idx, end_idx))\n",
    "    \n",
    "    print(f\"\\nEvaluating chunk {i + 1}/{num_chunks} (Samples {start_idx}-{end_idx})\")\n",
    "    \n",
    "    # Evaluate the chunk\n",
    "    eval_results = trainer.evaluate(val_dataset_chunk)\n",
    "    print(f\"Chunk {i + 1} Results: {eval_results}\")\n",
    "    \n",
    "    # Save the metrics for this chunk\n",
    "    all_metrics.append(eval_results)\n",
    "\n",
    "# Aggregate results\n",
    "final_metrics = {}\n",
    "for key in all_metrics[0].keys():\n",
    "    if isinstance(all_metrics[0][key], (int, float)):  # Aggregate numerical metrics\n",
    "        final_metrics[key] = sum(d[key] for d in all_metrics) / len(all_metrics)\n",
    "\n",
    "print(\"\\nFinal Evaluation Metrics for the Entire Dataset:\")\n",
    "for key, value in final_metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Custom evaluation loop to handle alignment and detailed metrics\n",
    "def custom_evaluation_loop(model, tokenizer, dataset, device=\"cuda\"):\n",
    "    \"\"\"Evaluate model and compute precision, recall, and F1.\"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_true_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        # Prepare input tensors\n",
    "        input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "        true_labels = np.array(sample[\"labels\"])  # Convert to NumPy for easier filtering\n",
    "\n",
    "        # Filter out ignored tokens (-100)\n",
    "        valid_indices = true_labels != -100\n",
    "        true_labels = true_labels[valid_indices]\n",
    "\n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits.squeeze(0).cpu().numpy()\n",
    "        predictions = np.argmax(logits, axis=1)[valid_indices]\n",
    "\n",
    "        # Append results\n",
    "        all_true_labels.extend(true_labels)\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_true_labels, all_predictions, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    print(\"\\nDetailed Evaluation Metrics:\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_true_labels, all_predictions))\n",
    "\n",
    "# Evaluate using the custom loop\n",
    "# clear_memory()\n",
    "# print(\"\\nRunning Custom Evaluation Loop:\")\n",
    "# custom_evaluation_loop(model, tokenizer, val_dataset, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QFz1REZCzceR"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# Updated function for processing and mapping synsets with OOV handling\n",
    "def process_corpus_sections_incrementally_with_synsets(\n",
    "    xml_path, model, tokenizer, known_synset_embeddings, chunk_size=10000\n",
    "):\n",
    "    \"\"\"\n",
    "    Process large XML datasets incrementally with OOV handling for synset mapping.\n",
    "\n",
    "    Args:\n",
    "        xml_path (str): Path to the XML file.\n",
    "        model: Fine-tuned BERT model.\n",
    "        tokenizer: Tokenizer used with the model.\n",
    "        known_synset_embeddings (dict): Precomputed embeddings for known synsets.\n",
    "        chunk_size (int): Number of sentences per chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: List of DataFrames, one per processed chunk.\n",
    "    \"\"\"\n",
    "    print(f\"Processing <corpus> sections from: {xml_path}\")\n",
    "\n",
    "    corpus_dfs = []  # Store data frames for each <corpus> section\n",
    "    texts, labels = [], []\n",
    "    inside_corpus = False\n",
    "    inside_text = False\n",
    "    corpus_count = 0\n",
    "    chunk_counter = 0\n",
    "\n",
    "    # Open the file and read it line-by-line\n",
    "    with open(xml_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            # Check for the start of a <corpus> section\n",
    "            if \"<corpus\" in line:\n",
    "                inside_corpus = True\n",
    "                texts, labels = [], []  # Reset lists for a new <corpus>\n",
    "                corpus_count += 1\n",
    "                print(f\"\\nProcessing <corpus> section {corpus_count}...\")\n",
    "\n",
    "            # Process each line only if we're inside a <corpus> section\n",
    "            if inside_corpus:\n",
    "                if \"<text\" in line:\n",
    "                    inside_text = True  # Start of a new <text> element\n",
    "                    text_buffer = [line]  # Reset the buffer\n",
    "\n",
    "                elif inside_text:\n",
    "                    text_buffer.append(line)  # Accumulate lines within <text>\n",
    "\n",
    "                    if \"</text>\" in line:  # End of <text> element\n",
    "                        inside_text = False\n",
    "                        # Parse the accumulated <text> element\n",
    "                        text_xml = \"\".join(text_buffer)\n",
    "                        text_elem = ET.fromstring(\"<root>\" + text_xml + \"</root>\")  # Wrap for valid XML\n",
    "\n",
    "                        # Process each sentence in the <text> element\n",
    "                        for sentence in text_elem.findall(\".//sentence\"):\n",
    "                            sentence_text = []\n",
    "                            sentence_labels = []\n",
    "\n",
    "                            # Extract words from <wf> and <instance> elements\n",
    "                            for word_elem in sentence:\n",
    "                                if word_elem.tag == \"wf\":\n",
    "                                    sentence_text.append(word_elem.text)\n",
    "                                elif word_elem.tag == \"instance\":\n",
    "                                    sentence_text.append(word_elem.text)\n",
    "                                    # Reuse get_synset with OOV handling\n",
    "                                    synset = get_synset(\n",
    "                                        word_elem.attrib.get(\"lemma\", \"\"),\n",
    "                                        word_elem.attrib.get(\"pos\", \"\"),\n",
    "                                        model,\n",
    "                                        tokenizer,\n",
    "                                        known_synset_embeddings\n",
    "                                    )\n",
    "                                    sentence_labels.append({\n",
    "                                        \"id\": word_elem.attrib.get(\"id\", \"\"),\n",
    "                                        \"lemma\": word_elem.attrib.get(\"lemma\", \"\"),\n",
    "                                        \"pos\": word_elem.attrib.get(\"pos\", \"\"),\n",
    "                                        \"synset\": synset\n",
    "                                    })\n",
    "\n",
    "                            # Append extracted sentence data to texts and labels\n",
    "                            if sentence_text:\n",
    "                                texts.append(\" \".join(sentence_text))\n",
    "                                labels.append(sentence_labels)\n",
    "\n",
    "                        # Check if we've reached the chunk size limit\n",
    "                        if len(texts) >= chunk_size:\n",
    "                            # Save the chunk to a DataFrame and clear memory\n",
    "                            corpus_df = pd.DataFrame({\"text\": texts, \"labels\": labels})\n",
    "                            corpus_dfs.append(corpus_df)\n",
    "                            print(f\"Processed {chunk_counter + 1} chunks of {chunk_size} records.\")\n",
    "                            chunk_counter += 1\n",
    "                            texts, labels = [], []  # Reset lists for the next chunk\n",
    "\n",
    "            # Check for the end of a <corpus> section\n",
    "            if \"</corpus>\" in line and inside_corpus:\n",
    "                inside_corpus = False\n",
    "                # Save any remaining data after the last chunk\n",
    "                if texts and labels:\n",
    "                    corpus_df = pd.DataFrame({\"text\": texts, \"labels\": labels})\n",
    "                    corpus_dfs.append(corpus_df)\n",
    "                    print(f\"Final chunk for <corpus> section {corpus_count}.\")\n",
    "                    texts, labels = [], []  # Correctly reset lists for the next corpus section\n",
    "\n",
    "    return corpus_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# Updated function for processing and mapping synsets with OOV handling\n",
    "def process_corpus_sections_incrementally_with_synsets(\n",
    "    xml_path, model, tokenizer, known_synset_embeddings, chunk_size=10000, max_records=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Process large XML datasets incrementally with OOV handling for synset mapping.\n",
    "\n",
    "    Args:\n",
    "        xml_path (str): Path to the XML file.\n",
    "        model: Fine-tuned BERT model.\n",
    "        tokenizer: Tokenizer used with the model.\n",
    "        known_synset_embeddings (dict): Precomputed embeddings for known synsets.\n",
    "        chunk_size (int): Number of sentences per chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: List of DataFrames, one per processed chunk.\n",
    "    \"\"\"\n",
    "    print(f\"Processing <corpus> sections from: {xml_path}\")\n",
    "\n",
    "    corpus_dfs = []  # Store data frames for each <corpus> section\n",
    "    texts, labels = [], []\n",
    "    inside_corpus = False\n",
    "    inside_text = False\n",
    "    corpus_count = 0\n",
    "    chunk_counter = 0\n",
    "    total_records = 0  # Track the total number of records processeds\n",
    "\n",
    "    # Open the file and read it line-by-line\n",
    "    with open(xml_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            \n",
    "            # Stop processing if the total record limit is reached\n",
    "            if max_records is not None and total_records >= max_records:\n",
    "                print(f\"Reached the maximum record limit: {max_records}. Stopping processing.\")\n",
    "                break\n",
    "            \n",
    "            # Check for the start of a <corpus> section\n",
    "            if \"<corpus\" in line:\n",
    "                inside_corpus = True\n",
    "                texts, labels = [], []  # Reset lists for a new <corpus>\n",
    "                corpus_count += 1\n",
    "                print(f\"\\nProcessing <corpus> section {corpus_count}...\")\n",
    "\n",
    "            # Process each line only if we're inside a <corpus> section\n",
    "            if inside_corpus:\n",
    "                if \"<text\" in line:\n",
    "                    inside_text = True  # Start of a new <text> element\n",
    "                    text_buffer = [line]  # Reset the buffer\n",
    "\n",
    "                elif inside_text:\n",
    "                    text_buffer.append(line)  # Accumulate lines within <text>\n",
    "\n",
    "                    if \"</text>\" in line:  # End of <text> element\n",
    "                        inside_text = False\n",
    "                        # Parse the accumulated <text> element\n",
    "                        text_xml = \"\".join(text_buffer)\n",
    "                        text_elem = ET.fromstring(\"<root>\" + text_xml + \"</root>\")  # Wrap for valid XML\n",
    "\n",
    "                        # Process each sentence in the <text> element\n",
    "                        for sentence in text_elem.findall(\".//sentence\"):\n",
    "                            sentence_text = []\n",
    "                            sentence_labels = []\n",
    "\n",
    "                            # Extract words from <wf> and <instance> elements\n",
    "                            for word_elem in sentence:\n",
    "                                if word_elem.tag == \"wf\":\n",
    "                                    sentence_text.append(word_elem.text)\n",
    "                                elif word_elem.tag == \"instance\":\n",
    "                                    sentence_text.append(word_elem.text)\n",
    "                                    # Reuse get_synset with OOV handling\n",
    "                                    synset = get_synset(\n",
    "                                        word_elem.attrib.get(\"lemma\", \"\"),\n",
    "                                        word_elem.attrib.get(\"pos\", \"\"),\n",
    "                                        model,\n",
    "                                        tokenizer,\n",
    "                                        known_synset_embeddings\n",
    "                                    )\n",
    "                                    sentence_labels.append({\n",
    "                                        \"id\": word_elem.attrib.get(\"id\", \"\"),\n",
    "                                        \"lemma\": word_elem.attrib.get(\"lemma\", \"\"),\n",
    "                                        \"pos\": word_elem.attrib.get(\"pos\", \"\"),\n",
    "                                        \"synset\": synset\n",
    "                                    })\n",
    "\n",
    "                            # Append extracted sentence data to texts and labels\n",
    "                            if sentence_text:\n",
    "                                texts.append(\" \".join(sentence_text))\n",
    "                                labels.append(sentence_labels)\n",
    "\n",
    "                        # Check if we've reached the chunk size limit\n",
    "                        if len(texts) >= chunk_size:\n",
    "                            # Save the chunk to a DataFrame and clear memory\n",
    "                            corpus_df = pd.DataFrame({\"text\": texts, \"labels\": labels})\n",
    "                            corpus_dfs.append(corpus_df)\n",
    "                            print(f\"Processed {chunk_counter + 1} chunks of {chunk_size} records.\")\n",
    "                            chunk_counter += 1\n",
    "                            texts, labels = [], []  # Reset lists for the next chunk\n",
    "\n",
    "            # Check for the end of a <corpus> section\n",
    "            if \"</corpus>\" in line and inside_corpus:\n",
    "                break # My Testing Please remove later\n",
    "                inside_corpus = False\n",
    "                # Save any remaining data after the last chunk\n",
    "                if texts and labels:\n",
    "                    corpus_df = pd.DataFrame({\"text\": texts, \"labels\": labels})\n",
    "                    corpus_dfs.append(corpus_df)\n",
    "                    print(f\"Final chunk for <corpus> section {corpus_count}.\")\n",
    "                    texts, labels = [], []  # Correctly reset lists for the next corpus section\n",
    "\n",
    "    return corpus_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN-qbUN2_b_r",
    "outputId": "0f470c3d-acdb-4dd8-c171-89368a76e16e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing <corpus> sections from: ./WSD_Training_Corpora/SemCor+OMSTI/semcor+omsti.data.xml\n",
      "\n",
      "Processing <corpus> section 1...\n",
      "Processed 1 chunks of 5000 records.\n",
      "Processed 2 chunks of 5000 records.\n",
      "Processed 3 chunks of 5000 records.\n",
      "Processed 4 chunks of 5000 records.\n",
      "Processed 5 chunks of 5000 records.\n",
      "Processed 6 chunks of 5000 records.\n",
      "Processed 7 chunks of 5000 records.\n",
      "Test dataset preview:\n",
      "                                                text  \\\n",
      "0  How long has it been since you reviewed the ob...   \n",
      "1  Have you permitted it to become a giveaway pro...   \n",
      "2  What effort do you make to assess results of y...   \n",
      "3  Do you measure its relation to reduced absente...   \n",
      "4  Have you set specific objectives for your empl...   \n",
      "\n",
      "                                              labels  \n",
      "0  [{'id': 'd000.s000.t000', 'lemma': 'long', 'po...  \n",
      "1  [{'id': 'd000.s001.t000', 'lemma': 'permit', '...  \n",
      "2  [{'id': 'd000.s002.t000', 'lemma': 'effort', '...  \n",
      "3  [{'id': 'd000.s003.t000', 'lemma': 'measure', ...  \n",
      "4  [{'id': 'd000.s004.t000', 'lemma': 'set', 'pos...  \n"
     ]
    }
   ],
   "source": [
    "xml_path = \"./WSD_Training_Corpora/SemCor+OMSTI/semcor+omsti.data.xml\"\n",
    "# xml_path = \"cleaned_file.xml\"\n",
    "\n",
    "\n",
    "test_corpus_dfs = process_corpus_sections_incrementally_with_synsets(\n",
    "    xml_path, model, tokenizer, known_synset_embeddings, chunk_size=5000, max_records=30000\n",
    ")\n",
    "\n",
    "# Combine all chunks into a single DataFrame (if memory permits)\n",
    "combined_test_df = pd.concat(test_corpus_dfs, ignore_index=True)\n",
    "print(\"Test dataset preview:\")\n",
    "print(combined_test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DuXPSmu_-vp"
   },
   "outputs": [],
   "source": [
    "tokenized_test_data = preprocess_dataset(combined_test_df, tokenizer, synset_to_id)\n",
    "hf_test_dataset = convert_to_hf_dataset(tokenized_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkFR-CcDCmvL",
    "outputId": "645a1a8d-ad6b-4bea-b942-cb5bed2a4548"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Testing Dataset Sample:\n",
      "{'input_ids': [101, 2003, 2115, 13131, 4005, 5378, 2205, 2172, 2489, 9343, 2326, 2005, 5126, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, 9675, 12726, 10072, 1111, 16729, 18231, 672, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessed Testing Dataset Sample:\")\n",
    "print(hf_test_dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3NfD08SCyct"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 4\n",
      "/home/svaidya4/.local/lib/python3.9/site-packages/transformers/trainer.py:2504: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 12.825652122497559, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_perplexity': 5.02734375, 'eval_runtime': 43.0548, 'eval_samples_per_second': 23.226, 'eval_steps_per_second': 5.807, 'epoch': 15.0}\n"
     ]
    }
   ],
   "source": [
    "# fine_tuned_model = BertForTokenClassification.from_pretrained(\"./results\")\n",
    "clear_memory()\n",
    "test_results = trainer.evaluate(hf_test_dataset.select(range(1000)))\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUtZF-wyELhA"
   },
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments, Trainer\n",
    "# import torch\n",
    "\n",
    "# trainer = Trainer()\n",
    "\n",
    "# trainer.save_model(\"./best_model_20241202_145605\")\n",
    "\n",
    "from transformers import BertForTokenClassification, BertTokenizerFast\n",
    "\n",
    "# Load the model\n",
    "model_path = \"./best_model_20241202_175204\"\n",
    "model = BertForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_synset_details(word, synset_name):\n",
    "    \"\"\"\n",
    "    Fetch details of the synset: definition and examples.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to find the synset for.\n",
    "        synset_name (str): The WordNet synset name (e.g., 'dog.n.01').\n",
    "\n",
    "    Returns:\n",
    "        dict: Synset details including definition and examples.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        synset = wn.synset(synset_name)\n",
    "        return {\n",
    "            \"word\": word,\n",
    "            \"definition\": synset.definition(),\n",
    "            \"examples\": synset.examples()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching synset details: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "v5dWWbJxDCXl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "\n",
    "def predict_synsets(sentence, model, tokenizer, id_to_synset, max_length=128):\n",
    "    \"\"\"\n",
    "    Predict WordNet synsets for each word in a given sentence using the fine-tuned model.\n",
    "    \"\"\"\n",
    "    # Determine the device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move model to the device\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    inputs = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    # Move the tokenized inputs to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Get the word IDs for alignment\n",
    "    word_ids = inputs['input_ids'][0].tolist()  # Assuming batch size of 1\n",
    "\n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get predictions (logits -> argmax)\n",
    "    logits = outputs.logits\n",
    "    predictions = logits.argmax(dim=-1).squeeze().tolist()\n",
    "\n",
    "    # Map predictions to WordNet synsets\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    predicted_synsets = []\n",
    "    for word_id, pred in zip(word_ids, predictions):\n",
    "        if word_id is not None and pred in id_to_synset:\n",
    "            predicted_synsets.append(id_to_synset[pred])\n",
    "        else:\n",
    "            predicted_synsets.append(\"UNK\")  # Unknown or ignored token\n",
    "\n",
    "    # Align tokens with synsets\n",
    "    result = []\n",
    "    for token, synset in zip(tokens, predicted_synsets):\n",
    "        result.append((token, synset))\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Cuj0DL1pE_3t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Synsets:\n",
      "star: slave.v.01\n",
      "wars: slave.v.01\n",
      "is: slave.v.01\n",
      "a: bottomless.s.01\n",
      "good: neuromuscular.a.01\n",
      "movie: neuromuscular.a.01\n",
      "[('star', 'slave.v.01'), ('wars', 'slave.v.01'), ('is', 'slave.v.01'), ('a', 'bottomless.s.01'), ('good', 'neuromuscular.a.01'), ('movie', 'neuromuscular.a.01')]\n",
      "\n",
      "Predicted Synsets and Definitions:\n",
      "{'word': 'movie', 'definition': 'affecting or characteristic of both neural and muscular tissue', 'examples': []}\n"
     ]
    }
   ],
   "source": [
    "# Prompt user for input\n",
    "# sentence = input(\"Enter a sentence: Everything happens for its own good.\")\n",
    "# sentence = \"My money is in the bank\"\n",
    "# sentence = \"We saw ducks near the bank\"\n",
    "sentence = \"Star wars is a good movie\"\n",
    "\n",
    "word = \"movie\"\n",
    "\n",
    "\n",
    "# Predict synsets\n",
    "predictions = predict_synsets(sentence, model, tokenizer, id_to_synset)\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPredicted Synsets:\")\n",
    "for token, synset in predictions:\n",
    "    print(f\"{token}: {synset}\")\n",
    "\n",
    "print(predictions)\n",
    "    \n",
    "print(\"\\nPredicted Synsets and Definitions:\")\n",
    "for token, synset_name in predictions:\n",
    "    if(token == word):\n",
    "        definition = get_synset_details(token, synset_name)\n",
    "        print(definition)\n",
    "    # else:\n",
    "    #     print(\"No synset found\")\n",
    "    # if synset_name != \"UNK\":  # If the synset is valid\n",
    "    #     definition = get_synset_details(token, synset_name)\n",
    "    #     print(f\"{token}: {synset_name} - {definition}\")\n",
    "    # else:\n",
    "    #     print(f\"{token}: {synset_name} - No synset found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06a444a0f4994ee3b678e6d38e74d9f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09d7738d0e7449ee88c6d14a71ba047f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a995c7a035d441ea370cebe72eaf8f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0bc8a0e16cd64bcea716208adf605475": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "113a61d6c9d24dd5bb1ad7c68b60d340": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09d7738d0e7449ee88c6d14a71ba047f",
      "placeholder": "",
      "style": "IPY_MODEL_21c5b56432c145749e7f0f5b3664aee6",
      "value": "tokenizer.json:100%"
     }
    },
    "13c55af7d4aa41749834e95f24c847d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2726deb1ca1d4676986b1c69b3f52d74",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed87a50374dc4cf3906ba1bdffcab1cc",
      "value": 231508
     }
    },
    "1fb34004a1304c3f99053b68e0aa978e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21c5b56432c145749e7f0f5b3664aee6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "258c4abcb7614310b46626b3ad75f26c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2726deb1ca1d4676986b1c69b3f52d74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29c0a18d551d437ba46ad56f121ccf0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6b68842c581f4027a61756cd1712ccc5",
       "IPY_MODEL_c72903775d144d0e8531bc5206346fe5",
       "IPY_MODEL_61772e9e02874be48d30d6e2706c203a"
      ],
      "layout": "IPY_MODEL_2bb89fe85cdc420ba1622b5fcf297ddc"
     }
    },
    "2bb89fe85cdc420ba1622b5fcf297ddc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dcf4b880d984be8aa26095f45efb796": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3528559911c6447e8145c4f5da80f15a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0bc8a0e16cd64bcea716208adf605475",
      "placeholder": "",
      "style": "IPY_MODEL_7072ae437f054c679f1f638ddc8a5fcf",
      "value": "config.json:100%"
     }
    },
    "3ad62049d0514655912c801a3a6c33af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06a444a0f4994ee3b678e6d38e74d9f2",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f7c4addd8e2146b4b45e035c693cc558",
      "value": 570
     }
    },
    "419a7943ed37457c970370a10a84d916": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_113a61d6c9d24dd5bb1ad7c68b60d340",
       "IPY_MODEL_f5bc40fe5dd8412cab460d1feac02397",
       "IPY_MODEL_52b8ccf4212348f2a3cd7b4f049b5d6e"
      ],
      "layout": "IPY_MODEL_9831527b26bd4cd488155a98aa4ec661"
     }
    },
    "46929edf6ad54411a531ee7c60973bc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4a2829465b004a1496976ce2cdf30c26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52b8ccf4212348f2a3cd7b4f049b5d6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_922e5c71c5914e8eafefbd23c5c0dfe1",
      "placeholder": "",
      "style": "IPY_MODEL_2dcf4b880d984be8aa26095f45efb796",
      "value": "466k/466k[00:00&lt;00:00,26.1MB/s]"
     }
    },
    "59d0d5d8e2e24b32a51719a23b450c79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a88d0525d04e418498a95932e6cb9bc7",
      "placeholder": "",
      "style": "IPY_MODEL_6609b1ff7b054fadbb1a2f7039cab62a",
      "value": "model.safetensors:100%"
     }
    },
    "61772e9e02874be48d30d6e2706c203a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a995c7a035d441ea370cebe72eaf8f0",
      "placeholder": "",
      "style": "IPY_MODEL_e53607d484084e1ca67d5b2197900e75",
      "value": "48.0/48.0[00:00&lt;00:00,4.23kB/s]"
     }
    },
    "6609b1ff7b054fadbb1a2f7039cab62a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "687c1412605b456e81b657a66de40d68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b68842c581f4027a61756cd1712ccc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_961c3571d38e4c13a76d6246470f9497",
      "placeholder": "",
      "style": "IPY_MODEL_4a2829465b004a1496976ce2cdf30c26",
      "value": "tokenizer_config.json:100%"
     }
    },
    "7072ae437f054c679f1f638ddc8a5fcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77b2a19e25a24d90ad73a865505d68a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_59d0d5d8e2e24b32a51719a23b450c79",
       "IPY_MODEL_8352f29cbe2a42069fd1c2491e54355c",
       "IPY_MODEL_837cfc32f9a34ada8c810fe3ba9b889b"
      ],
      "layout": "IPY_MODEL_687c1412605b456e81b657a66de40d68"
     }
    },
    "833cf2599d574c79815f1abd963d2b4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_867c79cb14b24ad4a9001f480c03ceae",
      "placeholder": "",
      "style": "IPY_MODEL_a519dd3bafa941ea8c096dd957361baf",
      "value": "232k/232k[00:00&lt;00:00,5.15MB/s]"
     }
    },
    "8352f29cbe2a42069fd1c2491e54355c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_866d39cfef2243cf8921df0b6c056ed6",
      "max": 440449768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_84c0c32bf99045fd9a23bfcb55189242",
      "value": 440449768
     }
    },
    "837cfc32f9a34ada8c810fe3ba9b889b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc246ef0646b4c9eaa44b627eb35557c",
      "placeholder": "",
      "style": "IPY_MODEL_dc22f864fc8f4ec3a63d573ee2d785c1",
      "value": "440M/440M[00:02&lt;00:00,200MB/s]"
     }
    },
    "84c0c32bf99045fd9a23bfcb55189242": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "866d39cfef2243cf8921df0b6c056ed6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "867c79cb14b24ad4a9001f480c03ceae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "922e5c71c5914e8eafefbd23c5c0dfe1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "961c3571d38e4c13a76d6246470f9497": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9831527b26bd4cd488155a98aa4ec661": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b39f39e51cc43a4a85c09f02bef2baa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bc755cbdd6745d3a872a07229f913fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc5116f44b6249d2baa9c72fa1850f0c",
       "IPY_MODEL_13c55af7d4aa41749834e95f24c847d6",
       "IPY_MODEL_833cf2599d574c79815f1abd963d2b4a"
      ],
      "layout": "IPY_MODEL_9b39f39e51cc43a4a85c09f02bef2baa"
     }
    },
    "a0ff159a3dc548c5881db5b772f0c6e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a519dd3bafa941ea8c096dd957361baf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a88d0525d04e418498a95932e6cb9bc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9fe243d8cfe48a7900c9b8784f3f951": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed57552d689b405b9dc63eb2ebf89628",
      "placeholder": "",
      "style": "IPY_MODEL_bfe550841e0340efa726623d13d56d42",
      "value": "570/570[00:00&lt;00:00,46.1kB/s]"
     }
    },
    "bfe550841e0340efa726623d13d56d42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c05bcb16a4e74722a18f68407b9f9e79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3528559911c6447e8145c4f5da80f15a",
       "IPY_MODEL_3ad62049d0514655912c801a3a6c33af",
       "IPY_MODEL_a9fe243d8cfe48a7900c9b8784f3f951"
      ],
      "layout": "IPY_MODEL_e67f94e0b1fe4cdf9be788540ba5b921"
     }
    },
    "c72903775d144d0e8531bc5206346fe5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0ff159a3dc548c5881db5b772f0c6e2",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_258c4abcb7614310b46626b3ad75f26c",
      "value": 48
     }
    },
    "cc5116f44b6249d2baa9c72fa1850f0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fb34004a1304c3f99053b68e0aa978e",
      "placeholder": "",
      "style": "IPY_MODEL_fdfac33240d04a9b8ae1f279ef3f9e1e",
      "value": "vocab.txt:100%"
     }
    },
    "dbd76c78abf64bd7be9161ed777972bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc22f864fc8f4ec3a63d573ee2d785c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc246ef0646b4c9eaa44b627eb35557c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e53607d484084e1ca67d5b2197900e75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e67f94e0b1fe4cdf9be788540ba5b921": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed57552d689b405b9dc63eb2ebf89628": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed87a50374dc4cf3906ba1bdffcab1cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f5bc40fe5dd8412cab460d1feac02397": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbd76c78abf64bd7be9161ed777972bb",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46929edf6ad54411a531ee7c60973bc0",
      "value": 466062
     }
    },
    "f7c4addd8e2146b4b45e035c693cc558": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fdfac33240d04a9b8ae1f279ef3f9e1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
